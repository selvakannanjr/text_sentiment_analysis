{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Python_project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMOkNRL4bkBvSL4SMBewsl2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/selvakannanjr/text_sentiment_analysis/blob/master/Python_project-19PD31%2C19PD33.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlkXx3eGF54P",
        "colab_type": "code",
        "outputId": "b5549225-e1c5-4962-af70-75bed662829b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer \n",
        "from keras.utils import to_categorical\n",
        "\n",
        "tf=pd.read_csv(\"/content/prodata.csv\")\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(tf['content'].values,tf['sentiment'].values,random_state=0)\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "train_vectors = vectorizer.fit_transform(X_train)\n",
        "test_vectors = vectorizer.transform(X_test)\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras import layers\n",
        "\n",
        "intdim=train_vectors.shape[1]\n",
        "y_train=to_categorical(y_train)\n",
        "y_test=to_categorical(y_test)\n",
        "\n",
        "\n",
        "model=Sequential()\n",
        "model.add(layers.Dense(10,input_dim=intdim,activation='relu'))\n",
        "model.add(layers.Dense(3,activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history=model.fit(train_vectors,y_train,epochs=100,verbose=False,validation_data=(test_vectors,y_test),batch_size=20)\n",
        "\n",
        "loss,acc=model.evaluate(test_vectors,y_test,verbose=False)\n",
        "\n",
        "print(acc)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_3 (Dense)              (None, 10)                348630    \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 348,663\n",
            "Trainable params: 348,663\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "0.4593999981880188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dnze0P1re4Zc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        },
        "outputId": "f451a8ed-f51a-44ac-c65f-ff1bfb169c74"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#nltk.download('punkt')\n",
        "#nltk.download('stopwords') \n",
        "#nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "import re \n",
        "from bs4 import BeautifulSoup\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "train=pd.read_csv(\"/content/train.tsv\",sep='\\t')\n",
        "test=pd.read_csv(\"/content/test.tsv\",sep='\\t')\n",
        "\n",
        "lem=WordNetLemmatizer()\n",
        "def clean_text(frame):\n",
        "    processed=[]\n",
        "    for sentence in tqdm(frame['Phrase']):\n",
        "      sentence=sentence.lower()                 # Converting to lowercase\n",
        "      cleanr=re.compile('<.*?>@')\n",
        "      sentence=re.sub(cleanr, ' ', sentence)        #Removing HTML tags\n",
        "      sentence=re.sub(r'[?|!|\\'|\"|#]',r'',sentence)\n",
        "      sentence=re.sub(r'[.|,|)|(|\\|/]',r' ',sentence)        #Removing Punctuations\n",
        "      words=[lem.lemmatize(word) for word in sentence.split() if word not in stopwords.words('english')]\n",
        "      processed.append(words)\n",
        "    return processed\n",
        "\n",
        "#trs=clean_text(train)\n",
        "#tes=clean_text(test)\n",
        "\n",
        "#with open('/content/trainlist.txt','w') as filehandle:\n",
        " # json.dump(trs,filehandle)\n",
        "#with open('/content/testlist.txt','w') as filehandle:\n",
        " # json.dump(tes,filehandle)\n",
        "\n",
        "with open('/content/trainlist.txt','r') as filehandle:\n",
        "  trs=json.load(filehandle)\n",
        "with open('/content/testlist.txt','r') as filehandle:\n",
        "  tes=json.load(filehandle)\n",
        "\n",
        "#so far we've processed and loaded the data. now begins the prep for DL\n",
        "\n",
        "from keras.utils import to_categorical #to convert the target labels to OneHotEncoded form\n",
        "import random\n",
        "#from tensorflow import set_random_seed\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence #convert the tokens into sequences in order to be fed to the lstm\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.layers import Dense,Dropout,Embedding,LSTM\n",
        "from keras.callbacks import EarlyStopping #save time by terminating the learning when the model stops improving\n",
        "from keras.losses import categorical_crossentropy #loss function suitable for multi-label classification\n",
        "from keras.optimizers import Adam\n",
        "from keras.models import Sequential\n",
        "from keras.models import load_model\n",
        "\n",
        "#set_random_seed(343)\n",
        "random.seed(343) #common seed for all pseudorandom integers in the following code\n",
        "\n",
        "tar=train.Sentiment.values\n",
        "y_target=to_categorical(tar)\n",
        "no_of_labels=y_target.shape[1]\n",
        "\n",
        "X_train,X_val,y_train,y_val=train_test_split(trs,y_target,random_state=0,test_size=0.2,stratify=y_target)\n",
        "unique_words = set()\n",
        "len_max = 0\n",
        "\n",
        "\n",
        "for sent in X_train:\n",
        "    \n",
        "    unique_words.update(sent)\n",
        "    \n",
        "    if(len_max<len(sent)):\n",
        "        len_max = len(sent)\n",
        "\n",
        "nw=len(list(unique_words))\n",
        "\n",
        "tokenizer=Tokenizer(num_words=nw)\n",
        "tokenizer.fit_on_texts(list(X_train))   #now the 'nw' no of most common words will be on the front of the list\n",
        "\n",
        "X_train=tokenizer.texts_to_sequences(X_train)           #most common words are convtd to sequences \n",
        "X_train=sequence.pad_sequences(X_train, maxlen=len_max) #since not all content are of same len,we pad those that are small with zeroes\n",
        "\n",
        "X_val=tokenizer.texts_to_sequences(X_val)\n",
        "X_val=sequence.pad_sequences(X_val, maxlen=len_max)\n",
        "\n",
        "X_test=tokenizer.texts_to_sequences(tes)\n",
        "X_test=sequence.pad_sequences(X_test, maxlen=len_max)\n",
        "\n",
        "early_stopping=EarlyStopping(min_delta=0.01,mode='max',monitor='val_accuracy',patience=3)\n",
        "callback=[early_stopping]\n",
        "'''\n",
        "model=Sequential()\n",
        "model.add(Embedding(nw,300,input_length=len_max))\n",
        "model.add(LSTM(128,dropout=0.5,recurrent_dropout=0.5,return_sequences=True))\n",
        "model.add(LSTM(64,dropout=0.5, recurrent_dropout=0.5,return_sequences=False))\n",
        "model.add(Dense(100,activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10,activation='relu'))\n",
        "model.add(Dense(no_of_labels,activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.05),metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "history=model.fit(X_train, y_train, validation_data=(X_val, y_val),epochs=6,batch_size=256,verbose=1,callbacks=callback)\n",
        "\n",
        "model.save(\"/content/model.h5\")\n",
        "'''\n",
        "model=load_model('model.h5')\n",
        "model.summary()\n",
        "ynew=model.predict_proba(X_test)\n",
        "\n",
        "for i in range(10,21):\n",
        "  print(\"X=%s, Predicted=%s\" % (X_test[i], ynew[i]))\n",
        "  \n",
        "    "
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/indexed_slices.py:434: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (None, 30, 300)           4494900   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (None, 30, 128)           219648    \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (None, 64)                49408     \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               6500      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 10)                1010      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 5)                 55        \n",
            "=================================================================\n",
            "Total params: 4,771,521\n",
            "Trainable params: 4,771,521\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0 486 641], Predicted=[0.05247055 0.16581115 0.47900906 0.23715566 0.06555356]\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0 486], Predicted=[0.04857063 0.15761454 0.5084401  0.22164235 0.06373247]\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0 641], Predicted=[0.05247055 0.16581115 0.47900906 0.23715566 0.06555356]\n",
            "X=[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0 193], Predicted=[0.05190499 0.1646482  0.48322132 0.23492147 0.06530402]\n",
            "X=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], Predicted=[0.05247055 0.16581115 0.47900906 0.23715566 0.06555356]\n",
            "X=[    0     0     0     0     0     0     0  3369    57    33   152   142\n",
            "  2548   198     1  8663  8664    12  6799    10  1423   132    67  1402\n",
            "   408 14167   594    14  1070    55], Predicted=[0.05247055 0.16581115 0.47900906 0.23715566 0.06555356]\n",
            "X=[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0 3369], Predicted=[0.05247055 0.16581115 0.47900906 0.23715566 0.06555356]\n",
            "X=[    0     0     0     0     0     0     0     0    57    33   152   142\n",
            "  2548   198     1  8663  8664    12  6799    10  1423   132    67  1402\n",
            "   408 14167   594    14  1070    55], Predicted=[0.05247055 0.16581115 0.47900906 0.23715566 0.06555356]\n",
            "X=[    0     0     0     0     0     0     0     0    57    33   152   142\n",
            "  2548   198     1  8663  8664    12  6799    10  1423   132    67  1402\n",
            "   408 14167   594    14  1070    55], Predicted=[0.05247055 0.16581115 0.47900906 0.23715566 0.06555356]\n",
            "X=[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
            "  0  0  0  0  0 57], Predicted=[0.05154705 0.16390772 0.48589692 0.23350474 0.06514357]\n",
            "X=[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0], Predicted=[0.05247055 0.16581115 0.47900906 0.23715566 0.06555356]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}